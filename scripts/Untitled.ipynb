{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating indices...\n",
      "Computing first degree...\n",
      "Computing second degree with combinations...\n",
      "Computing from degree 3 to 10 without combinations...\n",
      "Computing third degree with some combinations...\n"
     ]
    }
   ],
   "source": [
    "# Removing bothering data and centering\n",
    "tX[tX==-999] = 0\n",
    "m = np.mean(tX, axis=0)\n",
    "centered_tX = tX - m\n",
    "\n",
    "centered_tX[centered_tX==0] = float('nan')\n",
    "stdevtrain = np.nanstd(centered_tX, axis=0)\n",
    "centered_tX[centered_tX==float('nan')] = 0\n",
    "standardized_tX = centered_tX / stdevtrain\n",
    "\n",
    "d = len(standardized_tX[0])\n",
    "n = len(standardized_tX)\n",
    "\n",
    "indices_s_deg = []\n",
    "indices_t_deg = []\n",
    "\n",
    "print(\"Creating indices...\")\n",
    "# Creating indices for subsets of degree 2\n",
    "for i in range (d):\n",
    "    for t in range (i,d):\n",
    "        indices_s_deg.append([t, i])\n",
    "indices_s_deg = np.array(indices_s_deg).T\n",
    "\n",
    "# Creating indices for subsets of degree 3\n",
    "max_t_degree = 15\n",
    "for i in range (max_t_degree):\n",
    "    for t in range (i,max_t_degree):\n",
    "        for j in range(t,max_t_degree):\n",
    "            if not (i == t and i == j):\n",
    "                indices_t_deg.append([j, t, i])\n",
    "indices_t_deg = np.array(indices_t_deg).T\n",
    "\n",
    "degrees = range(3,11)\n",
    "degrees_number = len(degrees) + 1\n",
    "stdX_Ncols = standardized_tX.shape[1]\n",
    "indices_s_Ncols = indices_s_deg.shape[1]\n",
    "indices_t_Ncols = indices_t_deg.shape[1]\n",
    "\n",
    "number_of_rows = indices_s_Ncols + degrees_number * stdX_Ncols + indices_t_Ncols\n",
    "\n",
    "mat = np.zeros((n, number_of_rows))\n",
    "\n",
    "print(\"Computing first degree...\")\n",
    "# First degree\n",
    "mat[:, :stdX_Ncols] = standardized_tX\n",
    "\n",
    "print(\"Computing second degree with combinations...\")\n",
    "# Second degree gotten from indices\n",
    "mat[:,stdX_Ncols:stdX_Ncols + indices_s_Ncols] = standardized_tX[:, indices_s_deg[0]] * standardized_tX[:, indices_s_deg[1]]\n",
    "\n",
    "print(\"Computing from degree 3 to 10 without combinations...\")\n",
    "# Improve 3 to 10 degree\n",
    "for i in degrees:\n",
    "    start_index = indices_s_Ncols + (i - 2) * stdX_Ncols\n",
    "    end_index = start_index + stdX_Ncols\n",
    "    mat[:,start_index:end_index] = standardized_tX**i\n",
    "    \n",
    "print(\"Computing third degree with some combinations...\")\n",
    "# Third degree gotten from indices\n",
    "mat[:, number_of_rows - indices_t_Ncols: number_of_rows] = standardized_tX[:, indices_t_deg[0]] * standardized_tX[:, indices_t_deg[1]] * standardized_tX[:, indices_t_deg[2]]\n",
    "\n",
    "\n",
    "m2 = np.mean(mat, axis=0)\n",
    "centered_mat = mat - m2\n",
    "centered_mat[mat==0] = 0\n",
    "\n",
    "centered_mat[centered_mat==0] = float('nan')\n",
    "stdev = np.nanstd(centered_mat, axis=0)\n",
    "centered_mat[centered_mat==float('nan')] = 0\n",
    "standardized_mat = centered_mat / stdev\n",
    "\n",
    "num_samples = len(standardized_mat)\n",
    "tx = np.c_[np.ones(num_samples), standardized_mat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeing memory\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'centered_mat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0686104b5522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Freeing memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentered_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentered_tX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandardized_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandardized_tX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdX_Ncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_s_deg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_s_Ncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_t_deg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_t_Ncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_TRAIN_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'centered_mat' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Freeing memory\")\n",
    "del(centered_mat, centered_tX, standardized_mat, standardized_tX, stdX_Ncols)\n",
    "del(indices_s_deg, indices_s_Ncols, indices_t_deg, indices_t_Ncols)\n",
    "del(mat, DATA_TRAIN_PATH, ids, stdev)\n",
    "\n",
    "lens = [(x,len(x)) for x in set(dir()) - set(dir(__builtins__))]\n",
    "testout = sorted(lens, key=lambda l: l[1])\n",
    "print(testout[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=[i for i in range (200000,250000)]\n",
    "b=[i for i in range (150000,200000)]\n",
    "c=[i for i in range (100000,150000)]\n",
    "d=[i for i in range (50000,100000)]\n",
    "e=[i for i in range (0,50000)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1=np.delete(tx,a,axis=0)\n",
    "x2=np.delete(tx,b,axis=0)\n",
    "x3=np.delete(tx,c,axis=0)\n",
    "x4=np.delete(tx,d,axis=0)\n",
    "x5=np.delete(tx,e,axis=0)\n",
    "y1=np.delete(y,a,axis=0)\n",
    "y2=np.delete(y,b,axis=0)\n",
    "y3=np.delete(y,c,axis=0)\n",
    "y4=np.delete(y,d,axis=0)\n",
    "y5=np.delete(y,e,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=0.5, w0=-0.007834825870646768, w1=0.0038243239389538536\n",
      "Gradient Descent(1/499): loss=0.5137949403893357, w0=-0.015413134737386525, w1=0.0060998689725988795\n",
      "Gradient Descent(2/499): loss=0.7295876954000513, w0=-0.022799925630840365, w1=0.009714451513393747\n",
      "Gradient Descent(3/499): loss=1.6590524634615402, w0=-0.029894997860204567, w1=0.009095875281194214\n",
      "Gradient Descent(4/499): loss=5.333410498712674, w0=-0.036922331135942434, w1=0.015831211581113645\n",
      "Gradient Descent(5/499): loss=19.50050945358153, w0=-0.04346348341285404, w1=0.007302342230722243\n",
      "Gradient Descent(6/499): loss=73.20951999281024, w0=-0.050357015706319265, w1=0.02791455680568619\n",
      "Gradient Descent(7/499): loss=273.6824649801958, w0=-0.05599791393131586, w1=-0.00825942803412718\n",
      "Gradient Descent(8/499): loss=1010.5707515675881, w0=-0.06349210127900043, w1=0.06414056917020147\n",
      "Gradient Descent(9/499): loss=3677.979491405385, w0=-0.06693007428351443, w1=-0.07040921738392797\n",
      "Gradient Descent(10/499): loss=13186.340150975366, w0=-0.0775225442703738, w1=0.18577698864215797\n",
      "Gradient Descent(11/499): loss=46562.14418339966, w0=-0.07421306726719164, w1=-0.290751701247012\n",
      "Gradient Descent(12/499): loss=161920.35468932163, w0=-0.09629793826334337, w1=0.5957997829638225\n",
      "Gradient Descent(13/499): loss=554507.5649620361, w0=-0.07104394333663666, w1=-1.0344554430891146\n",
      "Gradient Descent(14/499): loss=1869958.0125930847, w0=-0.13208906512401575, w1=1.9465077408208753\n",
      "Gradient Descent(15/499): loss=6209491.998504941, w0=-0.0358084489493687, w1=-3.456588953714232\n",
      "Gradient Descent(16/499): loss=20303067.79162895, w0=-0.2229706873232315, w1=6.266732398885348\n",
      "Gradient Descent(17/499): loss=65362709.75946448, w0=0.09770215955730405, w1=-11.09076323306953\n",
      "Gradient Descent(18/499): loss=207177439.46811625, w0=-0.48340041639351855, w1=19.659727229372315\n",
      "Gradient Descent(19/499): loss=646516997.4236737, w0=0.5254156397946328, w1=-34.38970336336452\n",
      "Gradient Descent(20/499): loss=1986201706.0850356, w0=-1.246193302865034, w1=59.87612707200557\n",
      "Gradient Descent(21/499): loss=6006940226.274195, w0=1.8076144545293846, w1=-103.24079170437093\n",
      "Gradient Descent(22/499): loss=17883406885.97475, w0=-3.4467364324527807, w1=176.80805817984864\n",
      "Gradient Descent(23/499): loss=52407616991.7181, w0=5.49225300452513, w1=-300.2139981249811\n",
      "Gradient Descent(24/499): loss=151170057974.58517, w0=-9.624743405512053, w1=505.92300434702224\n",
      "Gradient Descent(25/499): loss=429184616013.1105, w0=15.709686120028035, w1=-845.6251685828881\n",
      "Gradient Descent(26/499): loss=1199248210035.0347, w0=-26.438691556433962, w1=1402.3846092639812\n",
      "Gradient Descent(27/499): loss=3297918497360.2217, w0=43.0988911297581, w1=-2306.961857040953\n",
      "Gradient Descent(28/499): loss=8925155239466.535, w0=-70.7365063184262, w1=3764.8647334338484\n",
      "Gradient Descent(29/499): loss=23769253434261.492, w0=114.10085227709484, w1=-6094.595153183808\n",
      "Gradient Descent(30/499): loss=62289931409510.65, w0=-183.63951004549793, w1=9786.785361842802\n",
      "Gradient Descent(31/499): loss=160620554440412.56, w0=292.084953624738, w1=-15588.667614370555\n",
      "Gradient Descent(32/499): loss=407514916840857.25, w0=-461.90469928630455, w1=24629.19156306461\n",
      "Gradient Descent(33/499): loss=1017238855393092.6, w0=723.4166338249713, w1=-38596.35533878296\n",
      "Gradient Descent(34/499): loss=2498141759121899.0, w0=-1124.8683471922873, w1=59991.54265132114\n",
      "Gradient Descent(35/499): loss=6035362221892318.0, w0=1733.6755043061034, w1=-92484.17030202305\n",
      "Gradient Descent(36/499): loss=1.4343623554708074e+16, w0=-2651.2002237030215, w1=141406.22194866365\n",
      "Gradient Descent(37/499): loss=3.3532087269627812e+16, w0=4019.810235685431, w1=-214427.5577353135\n",
      "Gradient Descent(38/499): loss=7.710545298958155e+16, w0=-6045.753621804553, w1=322472.39457568136\n",
      "Gradient Descent(39/499): loss=1.7438484867499427e+17, w0=9016.282430920555, w1=-480941.11651860346\n",
      "Gradient Descent(40/499): loss=3.8788915213624845e+17, w0=-13335.758755447981, w1=711323.1072043639\n",
      "Gradient Descent(41/499): loss=8.485111378229683e+17, w0=19558.94664696019, w1=-1043290.1520875017\n",
      "Gradient Descent(42/499): loss=1.8252989425144228e+18, w0=-28447.35192162897, w1=1517379.6077817506\n",
      "Gradient Descent(43/499): loss=3.861110426521282e+18, w0=41026.47677174282, w1=-2188374.690439006\n",
      "Gradient Descent(44/499): loss=8.03095398725521e+18, w0=-58670.65271484904, w1=3129498.989066195\n",
      "Gradient Descent(45/499): loss=1.6423789782423482e+19, w0=83192.52880591794, w1=-4437524.378101656\n",
      "Gradient Descent(46/499): loss=3.3022144059819033e+19, w0=-116963.92148486956, w1=6238878.615704367\n",
      "Gradient Descent(47/499): loss=6.5273488432707396e+19, w0=163043.34274326276, w1=-8696790.183640098\n",
      "Gradient Descent(48/499): loss=1.2683576872815364e+20, w0=-225335.62514807086, w1=12019456.49551657\n",
      "Gradient Descent(49/499): loss=2.4226652702306242e+20, w0=308755.63544035726, w1=-16469126.357048245\n",
      "Gradient Descent(50/499): loss=4.54846525830887e+20, w0=-419418.3289610096, w1=22371883.993278332\n",
      "Gradient Descent(51/499): loss=8.393225133540799e+20, w0=564821.8236257853, w1=-30127771.31155408\n",
      "Gradient Descent(52/499): loss=1.5221511143715066e+21, w0=-754040.5151942302, w1=40220728.254791684\n",
      "Gradient Descent(53/499): loss=2.7128364714724693e+21, w0=997887.7791944104, w1=-53227635.18415762\n",
      "Gradient Descent(54/499): loss=4.751142983098312e+21, w0=-1309058.642580459, w1=69825555.94934183\n",
      "Gradient Descent(55/499): loss=8.176219847877382e+21, w0=1702204.2744988562, w1=-90796081.50859186\n",
      "Gradient Descent(56/499): loss=1.382477172942085e+22, w0=-2193942.58775229, w1=117025522.04591176\n",
      "Gradient Descent(57/499): loss=2.296597726931501e+22, w0=2802751.2252103966, w1=-149499575.23564517\n",
      "Gradient Descent(58/499): loss=3.74803531511981e+22, w0=-3548745.052635057, w1=189291078.28470594\n",
      "Gradient Descent(59/499): loss=6.008748777055921e+22, w0=4453284.891209406, w1=-237539519.8622892\n",
      "Gradient Descent(60/499): loss=9.462273685049776e+22, w0=-5538425.553483698, w1=295421213.10605663\n",
      "Gradient Descent(61/499): loss=1.4635484726946634e+23, w0=6826160.4799275445, w1=-364109392.1422258\n",
      "Gradient Descent(62/499): loss=2.2232460017758303e+23, w0=-8337488.5411476465, w1=444724044.52998906\n",
      "Gradient Descent(63/499): loss=3.3166907378579994e+23, w0=10091283.196983136, w1=-538271985.0217376\n",
      "Gradient Descent(64/499): loss=4.858780846040578e+23, w0=-12103019.023224164, w1=645578532.4212909\n",
      "Gradient Descent(65/499): loss=6.989110664026345e+23, w0=14383368.983526688, w1=-767213085.8743291\n",
      "Gradient Descent(66/499): loss=9.87087745083319e+23, w0=-16936763.948475346, w1=903411888.5365002\n",
      "Gradient Descent(67/499): loss=1.3686593207595464e+24, w0=19759963.346133642, w1=-1054002188.7113273\n",
      "Gradient Descent(68/499): loss=1.8629739495091215e+24, w0=-22840761.05984985, w1=1218332806.6751473\n",
      "Gradient Descent(69/499): loss=2.489176167749984e+24, w0=26156899.92089642, w1=-1395216641.0717256\n",
      "Gradient Descent(70/499): loss=3.2644276801603624e+24, w0=-29675332.78697548, w1=1582890844.7221644\n",
      "Gradient Descent(71/499): loss=4.2017062910774667e+24, w0=33351901.95377472, w1=-1779000136.4278042\n",
      "Gradient Descent(72/499): loss=5.307325445372403e+24, w0=-37131556.51013929, w1=1980607980.5436938\n",
      "Gradient Descent(73/499): loss=6.578408006927786e+24, w0=40949141.52485764, w1=-2184239098.7281017\n",
      "Gradient Descent(74/499): loss=8.000628600592911e+24, w0=-44730822.948728114, w1=2385955056.298917\n",
      "Gradient Descent(75/499): loss=9.546590061256656e+24, w0=48396109.50545366, w1=-2581462530.8027\n",
      "Gradient Descent(76/499): loss=1.1175202671121904e+25, w0=-51860451.43822265, w1=2766251507.643937\n",
      "Gradient Descent(77/499): loss=1.2832376221818526e+25, w0=55038286.82222994, w1=-2935758195.44765\n",
      "Gradient Descent(78/499): loss=1.4453210252993285e+25, w0=-57846426.85520618, w1=3085545172.4083347\n",
      "Gradient Descent(79/499): loss=1.5965685656542765e+25, w0=60207571.556530595, w1=-3211489342.5501165\n",
      "Gradient Descent(80/499): loss=1.7295643575729858e+25, w0=-62053786.475197405, w1=3309966954.7078304\n",
      "Gradient Descent(81/499): loss=1.837261915066967e+25, w0=63329695.57101187, w1=-3378024343.1075125\n",
      "Gradient Descent(82/499): loss=1.9135918225894524e+25, w0=-63995215.559200704, w1=3413523345.0262775\n",
      "Gradient Descent(83/499): loss=1.954022279807025e+25, w0=64027614.185553506, w1=-3415251524.5265923\n",
      "Gradient Descent(84/499): loss=1.956001323691266e+25, w0=-63422778.28416151, w1=3382989374.6521025\n",
      "Gradient Descent(85/499): loss=1.9192211638553256e+25, w0=62195563.22290543, w1=-3317529394.918086\n",
      "Gradient Descent(86/499): loss=1.845666866596645e+25, w0=-60379221.56435169, w1=3220645176.390437\n",
      "Gradient Descent(87/499): loss=1.739440259605714e+25, w0=58023907.178524256, w1=-3095012051.682104\n",
      "Gradient Descent(88/499): loss=1.6063805812881004e+25, w0=-55194380.0924986, w1=2944084228.4139967\n",
      "Gradient Descent(89/499): loss=1.4535307305521363e+25, w0=51967026.43688917, w1=-2771936276.3123255\n",
      "Gradient Descent(90/499): loss=1.2885172612953706e+25, w0=-48426418.0031379, w1=2583079167.545294\n",
      "Gradient Descent(91/499): loss=1.118920395834562e+25, w0=44661596.6310056, w1=-2382262531.7307262\n",
      "Gradient Descent(92/499): loss=9.517064157178403e+24, w0=-40762345.528428435, w1=2174275318.8236933\n",
      "Gradient Descent(93/499): loss=7.927802614612142e+24, w0=36815633.82245976, w1=-1963756600.0109677\n",
      "Gradient Descent(94/499): loss=6.466943579291198e+24, w0=-32902463.04287356, w1=1755026907.3354306\n",
      "Gradient Descent(95/499): loss=5.165249628257537e+24, w0=29095237.663663417, w1=-1551948429.8040185\n",
      "Gradient Descent(96/499): loss=4.039041619798749e+24, w0=-25455801.50451026, w1=1357819821.0215034\n",
      "Gradient Descent(97/499): loss=3.091776164355909e+24, w0=22034160.761981152, w1=-1175308539.9932113\n",
      "Gradient Descent(98/499): loss=2.3164745878668688e+24, w0=-18867930.231754765, w1=1006420856.5561738\n",
      "Gradient Descent(99/499): loss=1.6985682939704216e+24, w0=15982423.070846848, w1=-852507096.4122229\n",
      "Gradient Descent(100/499): loss=1.2187644063308614e+24, w0=-13391333.947697785, w1=714297622.2980034\n",
      "Gradient Descent(101/499): loss=8.556224388439003e+23, w0=11097868.297026195, w1=-591963528.1602261\n",
      "Gradient Descent(102/499): loss=5.8764312093412374e+23, w0=-9096223.293594316, w1=485195174.2148113\n",
      "Gradient Descent(103/499): loss=3.947814173259438e+23, w0=7373252.739008632, w1=-393291454.0976834\n",
      "Gradient Descent(104/499): loss=2.5938972934810737e+23, w0=-5910223.947731378, w1=315253044.9953376\n",
      "Gradient Descent(105/499): loss=1.6666420559768543e+23, w0=4684520.272544999, w1=-249873684.5769428\n",
      "Gradient Descent(106/499): loss=1.0470436770173617e+23, w0=-3671233.729266964, w1=195824657.2982493\n",
      "Gradient Descent(107/499): loss=6.430703806139494e+22, w0=2844546.655341832, w1=-151728958.24185765\n",
      "Gradient Descent(108/499): loss=3.860652470325306e+22, w0=-2178896.3280074783, w1=116222947.42766538\n",
      "Gradient Descent(109/499): loss=2.265205076370596e+22, w0=1649869.667030403, w1=-88004541.24718997\n",
      "Gradient Descent(110/499): loss=1.298775010509091e+22, w0=-1234865.5823016926, w1=65868073.59394744\n",
      "Gradient Descent(111/499): loss=7.275681515062146e+21, w0=913507.8718408411, w1=-48726789.57819019\n",
      "Gradient Descent(112/499): loss=3.981615827871649e+21, w0=-667873.2610870132, w1=35624538.67858176\n",
      "Gradient Descent(113/499): loss=2.1282480963920333e+21, w0=482534.8647637402, w1=-25738564.43796824\n",
      "Gradient Descent(114/499): loss=1.1109445928278064e+21, w0=-344494.542440556, w1=18375423.982417643\n",
      "Gradient Descent(115/499): loss=5.662373217477887e+20, w0=243005.54521764186, w1=-12962001.113702998\n",
      "Gradient Descent(116/499): loss=2.8175255885764477e+20, w0=-169354.4859398232, w1=9033402.52129266\n",
      "Gradient Descent(117/499): loss=1.3684422568192264e+20, w0=116595.31896554207, w1=-6219243.032122226\n",
      "Gradient Descent(118/499): loss=6.486326718866599e+19, w0=-79293.62118999485, w1=4229529.825889558\n",
      "Gradient Descent(119/499): loss=2.9999079541240336e+19, w0=53262.07860256391, w1=-2841029.670446379\n",
      "Gradient Descent(120/499): loss=1.3535531422996009e+19, w0=-35334.31412973054, w1=1884727.5904698474\n",
      "Gradient Descent(121/499): loss=5.956908216843996e+18, w0=23147.76055863395, w1=-1234723.25808961\n",
      "Gradient Descent(122/499): loss=2.556601572967041e+18, w0=-14974.424827102564, w1=798725.1446077011\n",
      "Gradient Descent(123/499): loss=1.0698389661144027e+18, w0=9563.588334287571, w1=-510139.6163435916\n",
      "Gradient Descent(124/499): loss=4.364172918902045e+17, w0=-6030.684243297188, w1=321663.3841392392\n",
      "Gradient Descent(125/499): loss=1.7351094110932445e+17, w0=3753.2041020945617, w1=-200212.07912997273\n",
      "Gradient Descent(126/499): loss=6.7220933762889656e+16, w0=-2306.261297313328, w1=123001.54184231476\n",
      "Gradient Descent(127/499): loss=2.537140320209285e+16, w0=1397.885142003774, w1=-74578.72405728145\n",
      "Gradient Descent(128/499): loss=9327264296344362.0, w0=-836.8553749205466, w1=44622.963138239065\n",
      "Gradient Descent(129/499): loss=3339177572884232.5, w0=493.6096478169868, w1=-26344.446816571275\n",
      "Gradient Descent(130/499): loss=1163867958405943.5, w0=-287.9638484714727, w1=15344.890752178311\n",
      "Gradient Descent(131/499): loss=394864673133189.5, w0=165.01247038920877, w1=-8817.016531071069\n",
      "Gradient Descent(132/499): loss=130368429758760.92, w0=-93.97124101152326, w1=4997.231033580625\n",
      "Gradient Descent(133/499): loss=41876764578864.65, w0=52.0799515736922, w1=-2793.200319965723\n",
      "Gradient Descent(134/499): loss=13084132826101.248, w0=-29.152172834484247, w1=1539.7267017030535\n",
      "Gradient Descent(135/499): loss=3975393202989.7935, w0=15.400548635660812, w1=-836.7461175708377\n",
      "Gradient Descent(136/499): loss=1174270743699.1646, w0=-8.693592042229238, w1=448.42491327013613\n",
      "Gradient Descent(137/499): loss=337129288539.74054, w0=4.15166060136252, w1=-236.76170270213072\n",
      "Gradient Descent(138/499): loss=94047927286.84113, w0=-2.5995911877975546, w1=123.33521156708412\n",
      "Gradient Descent(139/499): loss=25486377152.52506, w0=0.8969489737281982, w1=-63.18777517050569\n",
      "Gradient Descent(140/499): loss=6707363903.41586, w0=-0.8883364060803045, w1=32.0236320643092\n",
      "Gradient Descent(141/499): loss=1713784824.4774768, w0=0.009140954403437651, w1=-15.864021675776833\n",
      "Gradient Descent(142/499): loss=425005018.4275162, w0=-0.43601372162946656, w1=7.864982640659591\n",
      "Gradient Descent(143/499): loss=102266794.06991771, w0=-0.2191714285841814, w1=-3.716853277386061\n",
      "Gradient Descent(144/499): loss=23869547.928201426, w0=-0.32383683047349365, w1=1.8508890115772\n",
      "Gradient Descent(145/499): loss=5402383.618598076, w0=-0.2747073775750001, w1=-0.7845718017613841\n",
      "Gradient Descent(146/499): loss=1185270.5008157098, w0=-0.29801238498480986, w1=0.4438962398529964\n",
      "Gradient Descent(147/499): loss=251997.35439408082, w0=-0.28771845498727855, w1=-0.11956424912949237\n",
      "Gradient Descent(148/499): loss=51900.751140858505, w0=-0.2927565553945092, w1=0.13503367043649983\n",
      "Gradient Descent(149/499): loss=10351.519944017862, w0=-0.2908990027952812, w1=0.0220539623827513\n",
      "Gradient Descent(150/499): loss=1998.773228531311, w0=-0.2920840946213977, w1=0.0716034541122259\n",
      "Gradient Descent(151/499): loss=373.65465623773525, w0=-0.29193929022423265, w1=0.050444832329696807\n",
      "Gradient Descent(152/499): loss=67.75983796965409, w0=-0.2923575500550757, w1=0.059544539784399934\n",
      "Gradient Descent(153/499): loss=12.076157403924235, w0=-0.29253290444566193, w1=0.05590741115716262\n",
      "Gradient Descent(154/499): loss=2.277057030733913, w0=-0.2928030249328748, w1=0.05754053919749312\n",
      "Gradient Descent(155/499): loss=0.6106444737299742, w0=-0.2930289627493597, w1=0.057028053165878514\n",
      "Gradient Descent(156/499): loss=0.3368717743685261, w0=-0.2932670873451209, w1=0.05737251182159492\n",
      "Gradient Descent(157/499): loss=0.29340563717025586, w0=-0.29349507425597987, w1=0.05737901429013055\n",
      "Gradient Descent(158/499): loss=0.28670668145254635, w0=-0.29372175982223253, w1=0.0575148801276395\n",
      "Gradient Descent(159/499): loss=0.28567200819480126, w0=-0.29394385146812835, w1=0.05760056065341011\n",
      "Gradient Descent(160/499): loss=0.28547945631510435, w0=-0.29416269172174897, w1=0.05770383428151383\n",
      "Gradient Descent(161/499): loss=0.28540807456082423, w0=-0.2943778800582707, w1=0.05779959150624514\n",
      "Gradient Descent(162/499): loss=0.2853539601504802, w0=-0.29458965663150866, w1=0.05789698161422498\n",
      "Gradient Descent(163/499): loss=0.2853026779465352, w0=-0.2947980274713826, w1=0.05799274830614779\n",
      "Gradient Descent(164/499): loss=0.28525228841670663, w0=-0.2950030797581118, w1=0.05808804441698574\n",
      "Gradient Descent(165/499): loss=0.2852025325008145, w0=-0.2952048703659571, w1=0.058182484768790066\n",
      "Gradient Descent(166/499): loss=0.2851533688224887, w0=-0.29540346459730005, w1=0.058276210276532216\n",
      "Gradient Descent(167/499): loss=0.28510478332682004, w0=-0.29559892304272156, w1=0.05836918513491449\n",
      "Gradient Descent(168/499): loss=0.28505676545121816, w0=-0.2957913060162677, w1=0.05846143152424258\n",
      "Gradient Descent(169/499): loss=0.285009305252389, w0=-0.2959806721488854, w1=0.05855295276329905\n",
      "Gradient Descent(170/499): loss=0.28496239306766424, w0=-0.2961670788840622, w1=0.0586437579766194\n",
      "Gradient Descent(171/499): loss=0.2849160194709303, w0=-0.29635058236471595, w1=0.05873385433969698\n",
      "Gradient Descent(172/499): loss=0.2848701752616347, w0=-0.29653123750942745, w1=0.05882324947109321\n",
      "Gradient Descent(173/499): loss=0.28482485145756103, w0=-0.29670909802949025, w1=0.058911950713658755\n",
      "Gradient Descent(174/499): loss=0.28478003928821055, w0=-0.2968842164626339, w1=0.058999965349653065\n",
      "Gradient Descent(175/499): loss=0.28473573018845627, w0=-0.2970566442007903, w1=0.059087300541326666\n",
      "Gradient Descent(176/499): loss=0.28469191579242803, w0=-0.29722643151855305, w1=0.05917396335040034\n",
      "Gradient Descent(177/499): loss=0.2846485879276124, w0=-0.29739362760045673, w1=0.05925996073528551\n",
      "Gradient Descent(178/499): loss=0.28460573860916694, w0=-0.29755828056763023, w1=0.05934529955436858\n",
      "Gradient Descent(179/499): loss=0.28456336003442617, w0=-0.29772043750369953, w1=0.05942998656760596\n",
      "Gradient Descent(180/499): loss=0.28452144457761097, w0=-0.2978801444800013, w1=0.05951402843850586\n",
      "Gradient Descent(181/499): loss=0.2844799847847101, w0=-0.29803744658011655, w1=0.05959743173595522\n",
      "Gradient Descent(182/499): loss=0.28443897336854673, w0=-0.2981923879237477, w1=0.05968020293603117\n",
      "Gradient Descent(183/499): loss=0.28439840320401005, w0=-0.29834501168995675, w1=0.059762348423763545\n",
      "Gradient Descent(184/499): loss=0.28435826732345565, w0=-0.29849536013978417, w1=0.05984387449485774\n",
      "Gradient Descent(185/499): loss=0.28431855891225555, w0=-0.2986434746382659, w1=0.05992478735737696\n",
      "Gradient Descent(186/499): loss=0.2842792713045002, w0=-0.29878939567586627, w1=0.06000509313338516\n",
      "Gradient Descent(187/499): loss=0.28424039797885287, w0=-0.2989331628893437, w1=0.06008479786055166\n",
      "Gradient Descent(188/499): loss=0.28420193255453297, w0=-0.2990748150820653, w1=0.06016390749371833\n",
      "Gradient Descent(189/499): loss=0.2841638687874326, w0=-0.2992143902437866, w1=0.06024242790643029\n",
      "Gradient Descent(190/499): loss=0.2841262005663762, w0=-0.2993519255699118, w1=0.060320364892431026\n",
      "Gradient Descent(191/499): loss=0.2840889219094839, w0=-0.29948745748024896, w1=0.06039772416712284\n",
      "Gradient Descent(192/499): loss=0.28405202696066695, w0=-0.29962102163727516, w1=0.06047451136899359\n",
      "Gradient Descent(193/499): loss=0.28401550998623504, w0=-0.2997526529639254, w1=0.060550732061010436\n",
      "Gradient Descent(194/499): loss=0.28397936537160945, w0=-0.2998823856609185, w1=0.060626391731981664\n",
      "Gradient Descent(195/499): loss=0.28394358761814775, w0=-0.30001025322363395, w1=0.060701495797887195\n",
      "Gradient Descent(196/499): loss=0.28390817134006807, w0=-0.3001362884585512, w1=0.06077604960317878\n",
      "Gradient Descent(197/499): loss=0.28387311126146725, w0=-0.3002605234992652, w1=0.060850058422050594\n",
      "Gradient Descent(198/499): loss=0.2838384022134433, w0=-0.3003829898220889, w1=0.06092352745968101\n",
      "Gradient Descent(199/499): loss=0.2838040391313019, w0=-0.30050371826125516, w1=0.06099646185344635\n",
      "Gradient Descent(200/499): loss=0.28377001705184934, w0=-0.300622739023729, w1=0.061068866674107346\n",
      "Gradient Descent(201/499): loss=0.2837363311107756, w0=-0.30074008170364075, w1=0.06114074692696898\n",
      "Gradient Descent(202/499): loss=0.28370297654011967, w0=-0.30085577529635177, w1=0.061212107553014504\n",
      "Gradient Descent(203/499): loss=0.2836699486658035, w0=-0.3009698482121615, w1=0.06128295343001425\n",
      "Gradient Descent(204/499): loss=0.2836372429052544, w0=-0.30108232828966713, w1=0.061353289373609884\n",
      "Gradient Descent(205/499): loss=0.2836048547650903, w0=-0.30119324280878484, w1=0.061423120138374865\n",
      "Gradient Descent(206/499): loss=0.2835727798388859, w0=-0.30130261850344203, w1=0.061492450418851596\n",
      "Gradient Descent(207/499): loss=0.28354101380499497, w0=-0.30141048157394995, w1=0.061561284850565966\n",
      "Gradient Descent(208/499): loss=0.2835095524244466, w0=-0.30151685769906494, w1=0.061629628011019864\n",
      "Gradient Descent(209/499): loss=0.2834783915389052, w0=-0.30162177204774776, w1=0.06169748442066223\n",
      "Gradient Descent(210/499): loss=0.28344752706868365, w0=-0.30172524929062816, w1=0.06176485854383923\n",
      "Gradient Descent(211/499): loss=0.2834169550108263, w0=-0.30182731361118376, w1=0.061831754789724046\n",
      "Gradient Descent(212/499): loss=0.2833866714372392, w0=-0.3019279887166405, w1=0.06189817751322688\n",
      "Gradient Descent(213/499): loss=0.2833566724928824, w0=-0.3020272978486023, w1=0.06196413101588567\n",
      "Gradient Descent(214/499): loss=0.2833269543940105, w0=-0.3021252637934174, w1=0.06202961954673795\n",
      "Gradient Descent(215/499): loss=0.2832975134264679, w0=-0.30222190889228845, w1=0.06209464730317441\n",
      "Gradient Descent(216/499): loss=0.2832683459440319, w0=-0.3023172550511333, w1=0.062159218431774695\n",
      "Gradient Descent(217/499): loss=0.283239448366806, w0=-0.3024113237502034, w1=0.06222333702912566\n",
      "Gradient Descent(218/499): loss=0.28321081717965435, w0=-0.3025041360534657, w1=0.06228700714262281\n",
      "Gradient Descent(219/499): loss=0.2831824489306871, w0=-0.3025957126177554, w1=0.06235023277125517\n",
      "Gradient Descent(220/499): loss=0.2831543402297847, w0=-0.3026860737017048, w1=0.06241301786637407\n",
      "Gradient Descent(221/499): loss=0.2831264877471669, w0=-0.3027752391744547, w1=0.06247536633244624\n",
      "Gradient Descent(222/499): loss=0.28309888821199963, w0=-0.302863228524154, w1=0.06253728202779162\n",
      "Gradient Descent(223/499): loss=0.28307153841104343, w0=-0.30295006086625326, w1=0.06259876876530623\n",
      "Gradient Descent(224/499): loss=0.28304443518733885, w0=-0.30303575495159757, w1=0.06265983031317056\n",
      "Gradient Descent(225/499): loss=0.28301757543892436, w0=-0.3031203291743239, w1=0.0627204703955437\n",
      "Gradient Descent(226/499): loss=0.28299095611760117, w0=-0.3032038015795686, w1=0.0627806926932437\n",
      "Gradient Descent(227/499): loss=0.28296457422771526, w0=-0.3032861898709896, w1=0.06284050084441448\n",
      "Gradient Descent(228/499): loss=0.2829384268249908, w0=-0.30336751141810814, w1=0.06289989844517943\n",
      "Gradient Descent(229/499): loss=0.28291251101538134, w0=-0.3034477832634754, w1=0.0629588890502824\n",
      "Gradient Descent(230/499): loss=0.2828868239539593, w0=-0.3035270221296676, w1=0.06301747617371589\n",
      "Gradient Descent(231/499): loss=0.2828613628438339, w0=-0.30360524442611525, w1=0.06307566328933735\n",
      "Gradient Descent(232/499): loss=0.2828361249350978, w0=-0.3036824662557698, w1=0.06313345383147323\n",
      "Gradient Descent(233/499): loss=0.2828111075238053, w0=-0.3037587034216124, w1=0.06319085119551159\n",
      "Gradient Descent(234/499): loss=0.2827863079509695, w0=-0.3038339714330091, w1=0.06324785873848329\n",
      "Gradient Descent(235/499): loss=0.2827617236015979, w0=-0.3039082855119161, w1=0.06330447977963209\n",
      "Gradient Descent(236/499): loss=0.28273735190374066, w0=-0.303981660598939, w1=0.0633607176009739\n",
      "Gradient Descent(237/499): loss=0.28271319032757614, w0=-0.3040541113592496, w1=0.0634165754478455\n",
      "Gradient Descent(238/499): loss=0.2826892363845123, w0=-0.30412565218836485, w1=0.06347205652944282\n",
      "Gradient Descent(239/499): loss=0.28266548762631405, w0=-0.3041962972177902, w1=0.06352716401934934\n",
      "Gradient Descent(240/499): loss=0.2826419416442515, w0=-0.30426606032053144, w1=0.0635819010560544\n",
      "Gradient Descent(241/499): loss=0.28261859606827544, w0=-0.30433495511647884, w1=0.06363627074346204\n",
      "Gradient Descent(242/499): loss=0.28259544856620816, w0=-0.3044029949776661, w1=0.06369027615139038\n",
      "Gradient Descent(243/499): loss=0.28257249684295654, w0=-0.30447019303340733, w1=0.06374392031606181\n",
      "Gradient Descent(244/499): loss=0.28254973863974764, w0=-0.30453656217531605, w1=0.06379720624058423\n",
      "Gradient Descent(245/499): loss=0.28252717173338343, w0=-0.3046021150622079, w1=0.06385013689542346\n",
      "Gradient Descent(246/499): loss=0.2825047939355074, w0=-0.3046668641248912, w1=0.06390271521886713\n",
      "Gradient Descent(247/499): loss=0.28248260309190193, w0=-0.3047308215708472, w1=0.06395494411748014\n",
      "Gradient Descent(248/499): loss=0.2824605970817911, w0=-0.3047939993888035, w1=0.06400682646655191\n",
      "Gradient Descent(249/499): loss=0.28243877381716936, w0=-0.3048564093532033, w1=0.06405836511053573\n",
      "Gradient Descent(250/499): loss=0.2824171312421425, w0=-0.3049180630285721, w1=0.06410956286348007\n",
      "Gradient Descent(251/499): loss=0.28239566733228605, w0=-0.3049789717737861, w1=0.06416042250945243\n",
      "Gradient Descent(252/499): loss=0.2823743800940205, w0=-0.3050391467462431, w1=0.06421094680295553\n",
      "Gradient Descent(253/499): loss=0.2823532675639993, w0=-0.3050985989059396, w1=0.06426113846933627\n",
      "Gradient Descent(254/499): loss=0.2823323278085149, w0=-0.30515733901945535, w1=0.06431100020518753\n",
      "Gradient Descent(255/499): loss=0.2823115589229158, w0=-0.30521537766384865, w1=0.06436053467874277\n",
      "Gradient Descent(256/499): loss=0.2822909590310421, w0=-0.3052727252304639, w1=0.06440974453026405\n",
      "Gradient Descent(257/499): loss=0.2822705262846691, w0=-0.3053293919286537, w1=0.06445863237242308\n",
      "Gradient Descent(258/499): loss=0.282250258862969, w0=-0.3053853877894181, w1=0.06450720079067582\n",
      "Gradient Descent(259/499): loss=0.28223015497198356, w0=-0.30544072266896205, w1=0.0645554523436306\n",
      "Gradient Descent(260/499): loss=0.282210212844107, w0=-0.3054954062521743, w1=0.06460338956340998\n",
      "Gradient Descent(261/499): loss=0.2821904307375874, w0=-0.3055494480560285, w1=0.06465101495600642\n",
      "Gradient Descent(262/499): loss=0.282170806936032, w0=-0.3056028574329092, w1=0.06469833100163186\n",
      "Gradient Descent(263/499): loss=0.28215133974793066, w0=-0.3056556435738641, w1=0.06474534015506149\n",
      "Gradient Descent(264/499): loss=0.2821320275061855, w0=-0.30570781551178466, w1=0.06479204484597165\n",
      "Gradient Descent(265/499): loss=0.2821128685676538, w0=-0.3057593821245168, w1=0.06483844747927209\n",
      "Gradient Descent(266/499): loss=0.2820938613127043, w0=-0.30581035213790303, w1=0.0648845504354327\n",
      "Gradient Descent(267/499): loss=0.282075004144777, w0=-0.30586073412875797, w1=0.06493035607080479\n",
      "Gradient Descent(268/499): loss=0.28205629548995925, w0=-0.3059105365277789, w1=0.06497586671793697\n",
      "Gradient Descent(269/499): loss=0.28203773379656727, w0=-0.30595976762239274, w1=0.06502108468588595\n",
      "Gradient Descent(270/499): loss=0.28201931753473947, w0=-0.306008435559541, w1=0.06506601226052214\n",
      "Gradient Descent(271/499): loss=0.28200104519603997, w0=-0.30605654834840434, w1=0.0651106517048302\n",
      "Gradient Descent(272/499): loss=0.28198291529306707, w0=-0.3061041138630679, w1=0.06515500525920481\n",
      "Gradient Descent(273/499): loss=0.281964926359074, w0=-0.30615113984512915, w1=0.06519907514174153\n",
      "Gradient Descent(274/499): loss=0.2819470769475984, w0=-0.3061976339062495, w1=0.06524286354852296\n",
      "Gradient Descent(275/499): loss=0.2819293656320958, w0=-0.30624360353065067, w1=0.06528637265390033\n",
      "Gradient Descent(276/499): loss=0.2819117910055886, w0=-0.30628905607755785, w1=0.06532960461077048\n",
      "Gradient Descent(277/499): loss=0.2818943516803118, w0=-0.3063339987835901, w1=0.06537256155084847\n",
      "Gradient Descent(278/499): loss=0.28187704628737986, w0=-0.30637843876509985, w1=0.06541524558493586\n",
      "Gradient Descent(279/499): loss=0.2818598734764498, w0=-0.30642238302046254, w1=0.06545765880318458\n",
      "Gradient Descent(280/499): loss=0.2818428319153976, w0=-0.30646583843231706, w1=0.06549980327535677\n",
      "Gradient Descent(281/499): loss=0.28182592028999903, w0=-0.3065088117697592, w1=0.06554168105108049\n",
      "Gradient Descent(282/499): loss=0.28180913730361856, w0=-0.3065513096904881, w1=0.0655832941601013\n",
      "Gradient Descent(283/499): loss=0.28179248167690474, w0=-0.3065933387429075, w1=0.06562464461253002\n",
      "Gradient Descent(284/499): loss=0.2817759521474926, w0=-0.3066349053681826, w1=0.06566573439908657\n",
      "Gradient Descent(285/499): loss=0.28175954746971, w0=-0.3066760159022537, w1=0.06570656549133999\n",
      "Gradient Descent(286/499): loss=0.28174326641429376, w0=-0.3067166765778072, w1=0.06574713984194476\n",
      "Gradient Descent(287/499): loss=0.28172710776810955, w0=-0.3067568935262058, w1=0.06578745938487347\n",
      "Gradient Descent(288/499): loss=0.28171107033387793, w0=-0.30679667277937744, w1=0.06582752603564591\n",
      "Gradient Descent(289/499): loss=0.28169515292990704, w0=-0.306836020271666, w1=0.0658673416915546\n",
      "Gradient Descent(290/499): loss=0.2816793543898285, w0=-0.30687494184164227, w1=0.06590690823188691\n",
      "Gradient Descent(291/499): loss=0.28166367356234345, w0=-0.3069134432338779, w1=0.06594622751814382\n",
      "Gradient Descent(292/499): loss=0.2816481093109681, w0=-0.3069515301006826, w1=0.06598530139425522\n",
      "Gradient Descent(293/499): loss=0.2816326605137895, w0=-0.3069892080038049, w1=0.06602413168679215\n",
      "Gradient Descent(294/499): loss=0.2816173260632222, w0=-0.30702648241609837, w1=0.06606272020517567\n",
      "Gradient Descent(295/499): loss=0.2816021048657742, w0=-0.30706335872315293, w1=0.06610106874188267\n",
      "Gradient Descent(296/499): loss=0.2815869958418138, w0=-0.307099842224893, w1=0.06613917907264853\n",
      "Gradient Descent(297/499): loss=0.28157199792534304, w0=-0.3071359381371427, w1=0.06617705295666686\n",
      "Gradient Descent(298/499): loss=0.2815571100637749, w0=-0.307171651593159, w1=0.06621469213678612\n",
      "Gradient Descent(299/499): loss=0.2815423312177191, w0=-0.30720698764513366, w1=0.06625209833970344\n",
      "Gradient Descent(300/499): loss=0.28152766036076227, w0=-0.30724195126566417, w1=0.06628927327615554\n",
      "Gradient Descent(301/499): loss=0.2815130964792644, w0=-0.30727654734919535, w1=0.06632621864110677\n",
      "Gradient Descent(302/499): loss=0.2814986385721526, w0=-0.3073107807134311, w1=0.06636293611393446\n",
      "Gradient Descent(303/499): loss=0.2814842856507178, w0=-0.3073446561007178, w1=0.06639942735861153\n",
      "Gradient Descent(304/499): loss=0.281470036738421, w0=-0.3073781781794, w1=0.06643569402388642\n",
      "Gradient Descent(305/499): loss=0.2814558908706982, w0=-0.3074113515451481, w1=0.06647173774346044\n",
      "Gradient Descent(306/499): loss=0.28144184709477194, w0=-0.30744418072226, w1=0.06650756013616244\n",
      "Gradient Descent(307/499): loss=0.28142790446946603, w0=-0.307476670164936, w1=0.06654316280612108\n",
      "Gradient Descent(308/499): loss=0.2814140620650228, w0=-0.3075088242585288, w1=0.06657854734293456\n",
      "Gradient Descent(309/499): loss=0.2814003189629256, w0=-0.3075406473207678, w1=0.06661371532183788\n",
      "Gradient Descent(310/499): loss=0.2813866742557229, w0=-0.30757214360295926, w1=0.06664866830386776\n",
      "Gradient Descent(311/499): loss=0.28137312704685746, w0=-0.3076033172911628, w1=0.06668340783602512\n",
      "Gradient Descent(312/499): loss=0.2813596764504978, w0=-0.30763417250734415, w1=0.06671793545143535\n",
      "Gradient Descent(313/499): loss=0.2813463215913731, w0=-0.30766471331050516, w1=0.06675225266950621\n",
      "Gradient Descent(314/499): loss=0.28133306160461113, w0=-0.3076949436977914, w1=0.06678636099608355\n",
      "Gradient Descent(315/499): loss=0.28131989563558035, w0=-0.30772486760557793, w1=0.06682026192360481\n",
      "Gradient Descent(316/499): loss=0.2813068228397319, w0=-0.30775448891053375, w1=0.06685395693125036\n",
      "Gradient Descent(317/499): loss=0.2812938423824483, w0=-0.30778381143066513, w1=0.0668874474850927\n",
      "Gradient Descent(318/499): loss=0.2812809534388949, w0=-0.30781283892633854, w1=0.06692073503824364\n",
      "Gradient Descent(319/499): loss=0.28126815519386766, w0=-0.30784157510128357, w1=0.0669538210309993\n",
      "Gradient Descent(320/499): loss=0.28125544684165454, w0=-0.3078700236035761, w1=0.06698670689098322\n",
      "Gradient Descent(321/499): loss=0.2812428275858898, w0=-0.30789818802660274, w1=0.06701939403328745\n",
      "Gradient Descent(322/499): loss=0.281230296639415, w0=-0.30792607191000565, w1=0.0670518838606116\n",
      "Gradient Descent(323/499): loss=0.2812178532241454, w0=-0.30795367874060997, w1=0.0670841777634001\n",
      "Gradient Descent(324/499): loss=0.2812054965709321, w0=-0.3079810119533326, w1=0.0671162771199775\n",
      "Gradient Descent(325/499): loss=0.28119322591943147, w0=-0.3080080749320736, w1=0.06714818329668196\n",
      "Gradient Descent(326/499): loss=0.281181040517978, w0=-0.3080348710105905, w1=0.06717989764799681\n",
      "Gradient Descent(327/499): loss=0.28116893962345435, w0=-0.30806140347335553, w1=0.06721142151668048\n",
      "Gradient Descent(328/499): loss=0.28115692250116836, w0=-0.3080876755563966, w1=0.06724275623389454\n",
      "Gradient Descent(329/499): loss=0.28114498842473046, w0=-0.30811369044812204, w1=0.06727390311933004\n",
      "Gradient Descent(330/499): loss=0.28113313667593187, w0=-0.3081394512901294, w1=0.06730486348133215\n",
      "Gradient Descent(331/499): loss=0.28112136654463066, w0=-0.308164961177999, w1=0.06733563861702312\n",
      "Gradient Descent(332/499): loss=0.2811096773286306, w0=-0.3081902231620725, w1=0.06736622981242356\n",
      "Gradient Descent(333/499): loss=0.28109806833357043, w0=-0.30821524024821617, w1=0.06739663834257216\n",
      "Gradient Descent(334/499): loss=0.2810865388728122, w0=-0.30824001539857, w1=0.06742686547164373\n",
      "Gradient Descent(335/499): loss=0.28107508826732946, w0=-0.30826455153228255, w1=0.06745691245306572\n",
      "Gradient Descent(336/499): loss=0.28106371584559997, w0=-0.308288851526232, w1=0.06748678052963318\n",
      "Gradient Descent(337/499): loss=0.2810524209435011, w0=-0.30831291821573337, w1=0.06751647093362219\n",
      "Gradient Descent(338/499): loss=0.2810412029042038, w0=-0.30833675439523256, w1=0.06754598488690179\n",
      "Gradient Descent(339/499): loss=0.28103006107807016, w0=-0.3083603628189874, w1=0.06757532360104446\n",
      "Gradient Descent(340/499): loss=0.28101899482255505, w0=-0.3083837462017356, w1=0.06760448827743507\n",
      "Gradient Descent(341/499): loss=0.28100800350210325, w0=-0.3084069072193508, w1=0.06763348010737853\n",
      "Gradient Descent(342/499): loss=0.28099708648805655, w0=-0.30842984850948574, w1=0.06766230027220593\n",
      "Gradient Descent(343/499): loss=0.28098624315855575, w0=-0.3084525726722037, w1=0.06769094994337932\n",
      "Gradient Descent(344/499): loss=0.2809754728984466, w0=-0.30847508227059833, w1=0.06771943028259514\n",
      "Gradient Descent(345/499): loss=0.2809647750991883, w0=-0.3084973798314017, w1=0.06774774244188632\n",
      "Gradient Descent(346/499): loss=0.2809541491587637, w0=-0.3085194678455812, w1=0.06777588756372303\n",
      "Gradient Descent(347/499): loss=0.28094359448158807, w0=-0.3085413487689255, w1=0.06780386678111215\n",
      "Gradient Descent(348/499): loss=0.28093311047842173, w0=-0.30856302502261956, w1=0.06783168121769546\n",
      "Gradient Descent(349/499): loss=0.28092269656628577, w0=-0.30858449899380896, w1=0.06785933198784655\n",
      "Gradient Descent(350/499): loss=0.2809123521683748, w0=-0.3086057730361542, w1=0.0678868201967665\n",
      "Gradient Descent(351/499): loss=0.2809020767139762, w0=-0.3086268494703745, w1=0.06791414694057842\n",
      "Gradient Descent(352/499): loss=0.2808918696383859, w0=-0.3086477305847816, w1=0.06794131330642059\n",
      "Gradient Descent(353/499): loss=0.28088173038282754, w0=-0.3086684186358043, w1=0.06796832037253862\n",
      "Gradient Descent(354/499): loss=0.280871658394376, w0=-0.30868891584850267, w1=0.06799516920837638\n",
      "Gradient Descent(355/499): loss=0.28086165312587674, w0=-0.3087092244170737, w1=0.06802186087466568\n",
      "Gradient Descent(356/499): loss=0.2808517140358706, w0=-0.30872934650534717, w1=0.06804839642351497\n",
      "Gradient Descent(357/499): loss=0.280841840588517, w0=-0.30874928424727277, w1=0.06807477689849685\n",
      "Gradient Descent(358/499): loss=0.28083203225352116, w0=-0.30876903974739844, w1=0.06810100333473448\n",
      "Gradient Descent(359/499): loss=0.28082228850606045, w0=-0.30878861508134, w1=0.06812707675898694\n",
      "Gradient Descent(360/499): loss=0.2808126088267124, w0=-0.30880801229624216, w1=0.06815299818973347\n",
      "Gradient Descent(361/499): loss=0.2808029927013844, w0=-0.3088272334112317, w1=0.06817876863725676\n",
      "Gradient Descent(362/499): loss=0.28079343962124426, w0=-0.30884628041786205, w1=0.0682043891037251\n",
      "Gradient Descent(363/499): loss=0.2807839490826518, w0=-0.30886515528055003, w1=0.06822986058327359\n",
      "Gradient Descent(364/499): loss=0.28077452058709085, w0=-0.30888385993700496, w1=0.06825518406208428\n",
      "Gradient Descent(365/499): loss=0.28076515364110394, w0=-0.30890239629864985, w1=0.06828036051846542\n",
      "Gradient Descent(366/499): loss=0.28075584775622703, w0=-0.3089207662510354, w1=0.06830539092292964\n",
      "Gradient Descent(367/499): loss=0.28074660244892363, w0=-0.3089389716542462, w1=0.06833027623827122\n",
      "Gradient Descent(368/499): loss=0.2807374172405264, w0=-0.3089570143433003, w1=0.0683550174196424\n",
      "Gradient Descent(369/499): loss=0.28072829165716784, w0=-0.30897489612854107, w1=0.06837961541462878\n",
      "Gradient Descent(370/499): loss=0.28071922522972587, w0=-0.3089926187960227, w1=0.0684040711633238\n",
      "Gradient Descent(371/499): loss=0.2807102174937599, w0=-0.30901018410788855, w1=0.06842838559840228\n",
      "Gradient Descent(372/499): loss=0.2807012679894529, w0=-0.3090275938027429, w1=0.0684525596451931\n",
      "Gradient Descent(373/499): loss=0.28069237626155236, w0=-0.30904484959601647, w1=0.06847659422175105\n",
      "Gradient Descent(374/499): loss=0.2806835418593134, w0=-0.30906195318032487, w1=0.06850049023892767\n",
      "Gradient Descent(375/499): loss=0.2806747643364417, w0=-0.3090789062258215, w1=0.06852424860044143\n",
      "Gradient Descent(376/499): loss=0.2806660432510396, w0=-0.309095710380544, w1=0.0685478702029469\n",
      "Gradient Descent(377/499): loss=0.28065737816554814, w0=-0.30911236727075436, w1=0.06857135593610325\n",
      "Gradient Descent(378/499): loss=0.28064876864669563, w0=-0.3091288785012736, w1=0.06859470668264174\n",
      "Gradient Descent(379/499): loss=0.28064021426544467, w0=-0.30914524565581036, w1=0.0686179233184326\n",
      "Gradient Descent(380/499): loss=0.28063171459693725, w0=-0.309161470297284, w1=0.06864100671255101\n",
      "Gradient Descent(381/499): loss=0.28062326922044617, w0=-0.3091775539681418, w1=0.06866395772734231\n",
      "Gradient Descent(382/499): loss=0.2806148777193222, w0=-0.30919349819067077, w1=0.06868677721848647\n",
      "Gradient Descent(383/499): loss=0.2806065396809451, w0=-0.3092093044673045, w1=0.06870946603506176\n",
      "Gradient Descent(384/499): loss=0.2805982546966749, w0=-0.30922497428092416, w1=0.0687320250196077\n",
      "Gradient Descent(385/499): loss=0.2805900223618023, w0=-0.30924050909515466, w1=0.06875445500818732\n",
      "Gradient Descent(386/499): loss=0.2805818422754996, w0=-0.30925591035465566, w1=0.06877675683044851\n",
      "Gradient Descent(387/499): loss=0.28057371404077863, w0=-0.30927117948540767, w1=0.06879893130968491\n",
      "Gradient Descent(388/499): loss=0.280565637264439, w0=-0.309286317894993, w1=0.0688209792628959\n",
      "Gradient Descent(389/499): loss=0.28055761155702474, w0=-0.3093013269728724, w1=0.06884290150084597\n",
      "Gradient Descent(390/499): loss=0.28054963653277903, w0=-0.30931620809065646, w1=0.06886469882812336\n",
      "Gradient Descent(391/499): loss=0.2805417118096005, w0=-0.30933096260237297, w1=0.0688863720431981\n",
      "Gradient Descent(392/499): loss=0.2805338370089989, w0=-0.3093455918447291, w1=0.06890792193847926\n",
      "Gradient Descent(393/499): loss=0.28052601175605246, w0=-0.30936009713736984, w1=0.0689293493003717\n",
      "Gradient Descent(394/499): loss=0.28051823567936574, w0=-0.3093744797831315, w1=0.06895065490933197\n",
      "Gradient Descent(395/499): loss=0.2805105084110247, w0=-0.30938874106829123, w1=0.06897183953992378\n",
      "Gradient Descent(396/499): loss=0.2805028295865614, w0=-0.30940288226281243, w1=0.06899290396087267\n",
      "Gradient Descent(397/499): loss=0.28049519884490837, w0=-0.3094169046205858, w1=0.06901384893512012\n",
      "Gradient Descent(398/499): loss=0.28048761582836124, w0=-0.30943080937966644, w1=0.0690346752198771\n",
      "Gradient Descent(399/499): loss=0.2804800801825374, w0=-0.3094445977625071, w1=0.0690553835666769\n",
      "Gradient Descent(400/499): loss=0.28047259155634063, w0=-0.30945827097618744, w1=0.06907597472142743\n",
      "Gradient Descent(401/499): loss=0.2804651496019183, w0=-0.30947183021263946, w1=0.06909644942446294\n",
      "Gradient Descent(402/499): loss=0.2804577539746267, w0=-0.30948527664886916, w1=0.0691168084105951\n",
      "Gradient Descent(403/499): loss=0.2804504043329924, w0=-0.30949861144717467, w1=0.06913705240916358\n",
      "Gradient Descent(404/499): loss=0.28044310033867753, w0=-0.3095118357553607, w1=0.06915718214408599\n",
      "Gradient Descent(405/499): loss=0.2804358416564402, w0=-0.3095249507069492, w1=0.06917719833390729\n",
      "Gradient Descent(406/499): loss=0.2804286279541025, w0=-0.30953795742138707, w1=0.06919710169184864\n",
      "Gradient Descent(407/499): loss=0.28042145890251313, w0=-0.30955085700424995, w1=0.0692168929258558\n",
      "Gradient Descent(408/499): loss=0.2804143341755139, w0=-0.3095636505474429, w1=0.06923657273864676\n",
      "Gradient Descent(409/499): loss=0.28040725344990447, w0=-0.30957633912939775, w1=0.06925614182775915\n",
      "Gradient Descent(410/499): loss=0.28040021640540924, w0=-0.30958892381526726, w1=0.06927560088559681\n",
      "Gradient Descent(411/499): loss=0.28039322272464406, w0=-0.309601405657116, w1=0.0692949505994761\n",
      "Gradient Descent(412/499): loss=0.2803862720930852, w0=-0.3096137856941083, w1=0.06931419165167158\n",
      "Gradient Descent(413/499): loss=0.28037936419903287, w0=-0.3096260649526928, w1=0.06933332471946116\n",
      "Gradient Descent(414/499): loss=0.28037249873358405, w0=-0.3096382444467844, w1=0.06935235047517084\n",
      "Gradient Descent(415/499): loss=0.28036567539059803, w0=-0.30965032517794305, w1=0.06937126958621892\n",
      "Gradient Descent(416/499): loss=0.28035889386666646, w0=-0.30966230813554974, w1=0.0693900827151597\n",
      "Gradient Descent(417/499): loss=0.28035215386108314, w0=-0.30967419429697945, w1=0.06940879051972673\n",
      "Gradient Descent(418/499): loss=0.28034545507581343, w0=-0.3096859846277715, w1=0.06942739365287562\n",
      "Gradient Descent(419/499): loss=0.28033879721546423, w0=-0.3096976800817972, w1=0.06944589276282631\n",
      "Gradient Descent(420/499): loss=0.28033217998725485, w0=-0.30970928160142447, w1=0.06946428849310496\n",
      "Gradient Descent(421/499): loss=0.2803256031009895, w0=-0.30972079011768033, w1=0.06948258148258532\n",
      "Gradient Descent(422/499): loss=0.2803190662690263, w0=-0.3097322065504104, w1=0.06950077236552972\n",
      "Gradient Descent(423/499): loss=0.2803125692062512, w0=-0.30974353180843595, w1=0.06951886177162954\n",
      "Gradient Descent(424/499): loss=0.2803061116300484, w0=-0.3097547667897085, w1=0.06953685032604534\n",
      "Gradient Descent(425/499): loss=0.2802996932602753, w0=-0.30976591238146184, w1=0.06955473864944647\n",
      "Gradient Descent(426/499): loss=0.2802933138192337, w0=-0.3097769694603619, w1=0.0695725273580503\n",
      "Gradient Descent(427/499): loss=0.2802869730316438, w0=-0.30978793889265377, w1=0.06959021706366104\n",
      "Gradient Descent(428/499): loss=0.2802806706246167, w0=-0.30979882153430705, w1=0.06960780837370811\n",
      "Gradient Descent(429/499): loss=0.2802744063276309, w0=-0.3098096182311582, w1=0.06962530189128416\n",
      "Gradient Descent(430/499): loss=0.2802681798725044, w0=-0.30982032981905117, w1=0.0696426982151826\n",
      "Gradient Descent(431/499): loss=0.2802619909933708, w0=-0.3098309571239755, w1=0.06965999793993484\n",
      "Gradient Descent(432/499): loss=0.2802558394266533, w0=-0.3098415009622026, w1=0.06967720165584704\n",
      "Gradient Descent(433/499): loss=0.2802497249110415, w0=-0.3098519621404193, w1=0.06969430994903651\n",
      "Gradient Descent(434/499): loss=0.28024364718746597, w0=-0.30986234145586006, w1=0.06971132340146777\n",
      "Gradient Descent(435/499): loss=0.28023760599907355, w0=-0.30987263969643636, w1=0.06972824259098813\n",
      "Gradient Descent(436/499): loss=0.2802316010912069, w0=-0.3098828576408647, w1=0.06974506809136302\n",
      "Gradient Descent(437/499): loss=0.28022563221137764, w0=-0.30989299605879234, w1=0.06976180047231086\n",
      "Gradient Descent(438/499): loss=0.2802196991092452, w0=-0.30990305571092086, w1=0.06977844029953756\n",
      "Gradient Descent(439/499): loss=0.28021380153659253, w0=-0.30991303734912823, w1=0.06979498813477077\n",
      "Gradient Descent(440/499): loss=0.2802079392473064, w0=-0.30992294171658874, w1=0.06981144453579363\n",
      "Gradient Descent(441/499): loss=0.28020211199735356, w0=-0.3099327695478911, w1=0.06982781005647831\n",
      "Gradient Descent(442/499): loss=0.2801963195447563, w0=-0.3099425215691548, w1=0.06984408524681905\n",
      "Gradient Descent(443/499): loss=0.2801905616495769, w0=-0.3099521984981444, w1=0.06986027065296499\n",
      "Gradient Descent(444/499): loss=0.2801848380738908, w0=-0.30996180104438265, w1=0.06987636681725261\n",
      "Gradient Descent(445/499): loss=0.28017914858176746, w0=-0.30997132990926113, w1=0.06989237427823784\n",
      "Gradient Descent(446/499): loss=0.28017349293925226, w0=-0.30998078578614985, w1=0.06990829357072784\n",
      "Gradient Descent(447/499): loss=0.28016787091434, w0=-0.3099901693605047, w1=0.06992412522581243\n",
      "Gradient Descent(448/499): loss=0.2801622822769613, w0=-0.3099994813099735, w1=0.06993986977089524\n",
      "Gradient Descent(449/499): loss=0.28015672679895726, w0=-0.31000872230450044, w1=0.06995552772972453\n",
      "Gradient Descent(450/499): loss=0.28015120425406315, w0=-0.3100178930064288, w1=0.0699710996224237\n",
      "Gradient Descent(451/499): loss=0.28014571441788755, w0=-0.31002699407060214, w1=0.06998658596552144\n",
      "Gradient Descent(452/499): loss=0.2801402570678925, w0=-0.3100360261444642, w1=0.07000198727198167\n",
      "Gradient Descent(453/499): loss=0.2801348319833777, w0=-0.31004498986815676, w1=0.07001730405123308\n",
      "Gradient Descent(454/499): loss=0.2801294389454554, w0=-0.3100538858746167, w1=0.0700325368091984\n",
      "Gradient Descent(455/499): loss=0.28012407773703957, w0=-0.31006271478967096, w1=0.07004768604832345\n",
      "Gradient Descent(456/499): loss=0.28011874814282184, w0=-0.31007147723213047, w1=0.07006275226760582\n",
      "Gradient Descent(457/499): loss=0.280113449949256, w0=-0.31008017381388253, w1=0.0700777359626232\n",
      "Gradient Descent(458/499): loss=0.2801081829445398, w0=-0.3100888051399818, w1=0.07009263762556162\n",
      "Gradient Descent(459/499): loss=0.2801029469185962, w0=-0.3100973718087401, w1=0.0701074577452432\n",
      "Gradient Descent(460/499): loss=0.28009774166305834, w0=-0.3101058744118144, w1=0.07012219680715377\n",
      "Gradient Descent(461/499): loss=0.2800925669712501, w0=-0.31011431353429414, w1=0.07013685529347014\n",
      "Gradient Descent(462/499): loss=0.2800874226381692, w0=-0.3101226897547867, w1=0.0701514336830871\n",
      "Gradient Descent(463/499): loss=0.28008230846047283, w0=-0.31013100364550195, w1=0.07016593245164422\n",
      "Gradient Descent(464/499): loss=0.28007722423645803, w0=-0.31013925577233536, w1=0.07018035207155231\n",
      "Gradient Descent(465/499): loss=0.28007216976604704, w0=-0.31014744669495, w1=0.07019469301201958\n",
      "Gradient Descent(466/499): loss=0.2800671448507713, w0=-0.310155576966857, w1=0.07020895573907773\n",
      "Gradient Descent(467/499): loss=0.28006214929375395, w0=-0.3101636471354956, w1=0.07022314071560758\n",
      "Gradient Descent(468/499): loss=0.2800571828996964, w0=-0.31017165774231087, w1=0.0702372484013645\n",
      "Gradient Descent(469/499): loss=0.2800522454748602, w0=-0.31017960932283145, w1=0.0702512792530037\n",
      "Gradient Descent(470/499): loss=0.28004733682705313, w0=-0.31018750240674525, w1=0.07026523372410515\n",
      "Gradient Descent(471/499): loss=0.2800424567656153, w0=-0.3101953375179747, w1=0.07027911226519824\n",
      "Gradient Descent(472/499): loss=0.2800376051013992, w0=-0.3102031151747503, w1=0.07029291532378638\n",
      "Gradient Descent(473/499): loss=0.2800327816467616, w0=-0.3102108358896837, w1=0.07030664334437113\n",
      "Gradient Descent(474/499): loss=0.28002798621554414, w0=-0.3102185001698391, w1=0.07032029676847623\n",
      "Gradient Descent(475/499): loss=0.2800232186230584, w0=-0.31022610851680427, w1=0.0703338760346714\n",
      "Gradient Descent(476/499): loss=0.28001847868607654, w0=-0.31023366142675984, w1=0.07034738157859582\n",
      "Gradient Descent(477/499): loss=0.2800137662228119, w0=-0.3102411593905482, w1=0.07036081383298153\n",
      "Gradient Descent(478/499): loss=0.2800090810529072, w0=-0.310248602893741, w1=0.07037417322767642\n",
      "Gradient Descent(479/499): loss=0.2800044229974214, w0=-0.3102559924167059, w1=0.07038746018966716\n",
      "Gradient Descent(480/499): loss=0.27999979187881396, w0=-0.31026332843467197, w1=0.0704006751431018\n",
      "Gradient Descent(481/499): loss=0.27999518752093316, w0=-0.310270611417795, w1=0.07041381850931226\n",
      "Gradient Descent(482/499): loss=0.27999060974900314, w0=-0.31027784183122076, w1=0.07042689070683647\n",
      "Gradient Descent(483/499): loss=0.2799860583896086, w0=-0.3102850201351482, w1=0.07043989215144043\n",
      "Gradient Descent(484/499): loss=0.2799815332706824, w0=-0.31029214678489137, w1=0.07045282325613997\n",
      "Gradient Descent(485/499): loss=0.279977034221495, w0=-0.3102992222309406, w1=0.07046568443122231\n",
      "Gradient Descent(486/499): loss=0.2799725610726388, w0=-0.3103062469190225, w1=0.07047847608426752\n",
      "Gradient Descent(487/499): loss=0.27996811365601587, w0=-0.31031322129015965, w1=0.07049119862016963\n",
      "Gradient Descent(488/499): loss=0.27996369180482833, w0=-0.3103201457807289, w1=0.0705038524411576\n",
      "Gradient Descent(489/499): loss=0.2799592953535628, w0=-0.3103270208225193, w1=0.07051643794681615\n",
      "Gradient Descent(490/499): loss=0.27995492413797946, w0=-0.3103338468427886, w1=0.0705289555341063\n",
      "Gradient Descent(491/499): loss=0.2799505779951001, w0=-0.3103406242643197, w1=0.07054140559738578\n",
      "Gradient Descent(492/499): loss=0.27994625676319757, w0=-0.31034735350547576, w1=0.07055378852842921\n",
      "Gradient Descent(493/499): loss=0.27994196028178026, w0=-0.31035403498025477, w1=0.07056610471644811\n",
      "Gradient Descent(494/499): loss=0.2799376883915862, w0=-0.3103606690983431, w1=0.07057835454811075\n",
      "Gradient Descent(495/499): loss=0.2799334409345646, w0=-0.31036725626516876, w1=0.07059053840756173\n",
      "Gradient Descent(496/499): loss=0.2799292177538709, w0=-0.3103737968819533, w1=0.07060265667644149\n",
      "Gradient Descent(497/499): loss=0.279925018693852, w0=-0.3103802913457637, w1=0.07061470973390555\n",
      "Gradient Descent(498/499): loss=0.2799208436000368, w0=-0.31038674004956274, w1=0.07062669795664359\n",
      "Gradient Descent(499/499): loss=0.27991669231912325, w0=-0.3103931433822593, w1=0.0706386217188984\n"
     ]
    }
   ],
   "source": [
    "max_iters = 500\n",
    "gamma = 0.025\n",
    "gradient_losses, gradient_w = least_squares_GD(y1, x1, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, least_squares_w=least_squares(y1,x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lamb=0.00075\n",
    "ridge_w=ridge_regression(y1, x1, lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d1e33d8a55c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#reg_logistic_w=reg_irls(tX,y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreg_logistic_w1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg_irls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreg_logistic_w2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg_irls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-fced5f2fe1af>\u001b[0m in \u001b[0;36mreg_irls\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mSX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mXSX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlamda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#reg_logistic_w=reg_irls(tX,y)\n",
    "print(1)\n",
    "reg_logistic_w1=reg_irls(x1,y1)\n",
    "print(2)\n",
    "reg_logistic_w2=reg_irls(x2,y2)\n",
    "print(3)\n",
    "reg_logistic_w3=reg_irls(x3,y3)\n",
    "print(4)\n",
    "reg_logistic_w4=reg_irls(x4,y4)\n",
    "print(5)\n",
    "reg_logistic_w5=reg_irls(x5,y5)\n",
    "reg_logistic_w=(reg_logistic_w1+reg_logistic_w2+reg_logistic_w3+reg_logistic_w4+reg_logistic_w5)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7392\n",
      "0.739788\n",
      "0.740796\n",
      "0.74168\n",
      "0.74242\n",
      "0.74318\n",
      "0.743372\n",
      "0.74358\n",
      "0.743864\n",
      "0.744064\n",
      "0.74424\n",
      "0.744356\n",
      "0.744476\n",
      "0.744608\n",
      "0.744656\n",
      "0.744744\n",
      "0.744772\n",
      "0.74482\n",
      "0.744892\n",
      "0.744888\n"
     ]
    }
   ],
   "source": [
    "logistic_w=irls(tX,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeing memory\n"
     ]
    }
   ],
   "source": [
    "print(\"Freeing memory\")\n",
    "del(y, tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating indices...\n",
      "Computing first degree...\n",
      "Computing second degree with combinations...\n",
      "Computing from degree 3 to 10 without combinations...\n",
      "Computing third degree with some combinations...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b9d526d9f74e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Computing third degree with some combinations...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Third degree gotten from indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_rows\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mindices_t_Ncols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumber_of_rows\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardized_testx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_t_deg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstandardized_testx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_t_deg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstandardized_testx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_t_deg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mcentered_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#weights = reg_logistic_w\n",
    "weights = logistic_w\n",
    "del(logistic_w)\n",
    "testx = tX_test\n",
    "del(tX_test)\n",
    "#testx=np.delete(testx,[14,15,17,18,24,25,27,28],axis=1)\n",
    "testx[testx==-999] = 0\n",
    "#m2=np.mean(testx,axis=0)\n",
    "centered_testx = testx - m\n",
    "centered_testx[testx==-999] = 0\n",
    "#standardized_testx=centered_testx / np.std(centered_testx, axis=0)\n",
    "#centered_testx[centered_testx==0]=float('nan')\n",
    "#stdevtest=np.nanstd(centered_testx,axis=0);\n",
    "#centered_testx[centered_testx==float('nan')]=0\n",
    "standardized_testx = centered_testx / stdevtrain\n",
    "\n",
    "d = len(standardized_testx[0])\n",
    "n = len(standardized_testx)\n",
    "\n",
    "indices_s_deg = []\n",
    "indices_t_deg = []\n",
    "\n",
    "print(\"Creating indices...\")\n",
    "# Creating indices for subsets of degree 2\n",
    "for i in range (d):\n",
    "    for t in range (i,d):\n",
    "        indices_s_deg.append([t, i])\n",
    "indices_s_deg = np.array(indices_s_deg).T\n",
    "\n",
    "# Creating indices for subsets of degree 3\n",
    "max_t_degree = 15\n",
    "for i in range (max_t_degree):\n",
    "    for t in range (i,max_t_degree):\n",
    "        for j in range(t,max_t_degree):\n",
    "            if not (i == t and i == j):\n",
    "                indices_t_deg.append([j, t, i])\n",
    "indices_t_deg = np.array(indices_t_deg).T\n",
    "\n",
    "degrees = range(3,11)\n",
    "degrees_number = len(degrees) + 1\n",
    "stdX_Ncols = standardized_testx.shape[1]\n",
    "indices_s_Ncols = indices_s_deg.shape[1]\n",
    "indices_t_Ncols = indices_t_deg.shape[1]\n",
    "\n",
    "number_of_rows = indices_s_Ncols + degrees_number * stdX_Ncols + indices_t_Ncols\n",
    "\n",
    "mat = np.zeros((n, number_of_rows))\n",
    "\n",
    "print(\"Computing first degree...\")\n",
    "# First degree\n",
    "mat[:, :stdX_Ncols] = standardized_testx\n",
    "\n",
    "print(\"Computing second degree with combinations...\")\n",
    "# Second degree gotten from indices\n",
    "mat[:,stdX_Ncols:stdX_Ncols + indices_s_Ncols] = standardized_testx[:, indices_s_deg[0]] * standardized_testx[:, indices_s_deg[1]]\n",
    "\n",
    "print(\"Computing from degree 3 to 10 without combinations...\")\n",
    "# Improve 3 to 10 degree\n",
    "for i in degrees:\n",
    "    start_index = indices_s_Ncols + (i - 2) * stdX_Ncols\n",
    "    end_index = start_index + stdX_Ncols\n",
    "    mat[:,start_index:end_index] = standardized_testx**i\n",
    "    \n",
    "print(\"Computing third degree with some combinations...\")\n",
    "# Third degree gotten from indices\n",
    "mat[:, number_of_rows - indices_t_Ncols: number_of_rows] = standardized_testx[:, indices_t_deg[0]] * standardized_testx[:, indices_t_deg[1]] * standardized_testx[:, indices_t_deg[2]]  \n",
    "        \n",
    "centered_mat = mat - m2\n",
    "centered_mat[mat==0] = 0\n",
    "\n",
    "print(\"Freeing memory\")\n",
    "del(standardized_testx, stdX_Ncols)\n",
    "del(indices_s_deg, indices_s_Ncols, indices_t_deg, indices_t_Ncols)\n",
    "del(mat, DATA_TEST_PATH, testx, centered_testx)\n",
    "\n",
    "#centered_mat[centered_mat==0]=float('nan')\n",
    "#stdev=np.nanstd(centered_mat,axis=0);\n",
    "#centered_mat[centered_mat==float('nan')]=0\n",
    "standardized_testmat = centered_mat / stdev\n",
    "\n",
    "#tao=int(d*(d+1)/2+d)\n",
    "    \n",
    "#for i in range (n):\n",
    "#    for r in range (d):\n",
    "#        for monsoon in range (r,d):\n",
    "#            for t in range (monsoon,d): \n",
    "#                mat[i,tao]=standardized_tX[i,t]*standardized_tX[i,monsoon]*standardized_tX[i,r]\n",
    "#                tao=tao+1\n",
    "#    tao=int(d*(d+1)/2+d)  \n",
    "\n",
    "print(\"Freeing memory\")\n",
    "del(centered_mat, stdev)\n",
    "\n",
    "num_samples = len(standardized_testmat)\n",
    "final_testx = np.c_[np.ones(num_samples), standardized_testmat]\n",
    "tX_test = final_testx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/Users/alperkose/Desktop/deneme8.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = -predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aq=np.dot(testx,reg_logistic_w)  \n",
    "y_guess=np.sign(aq)\n",
    "trueness=sum((y_guess==testy))/len(testy)\n",
    "print(trueness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
