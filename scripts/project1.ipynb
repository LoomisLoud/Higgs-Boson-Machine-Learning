{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '/Users/alperkose/Desktop/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio,seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data based on the given ratio: TODO\n",
    "    \n",
    "    \n",
    "    c = list(zip(x, y))\n",
    "\n",
    "    np.random.shuffle(c)\n",
    "\n",
    "    x, y = zip(*c)\n",
    "    \n",
    "    ind=round(ratio*len(x))\n",
    "    ind2=len(x)\n",
    "    x=np.array(x)\n",
    "    trainx = x[0:ind-1,:]\n",
    "    trainy = y[0:ind-1]\n",
    "    testx = x[ind:ind2-1,:]\n",
    "    testy = y[ind:ind2-1]\n",
    "    \n",
    "    return trainx, testx,trainy,testy\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX,testx,y,testy=split_data(tX, y, 0.8,seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX[tX==-999]=0\n",
    "m=np.mean(tX,axis=0)\n",
    "centered_tX=tX-m\n",
    "centered_tX[centered_tX==-m]=0\n",
    "\n",
    "\n",
    "standardized_tX=centered_tX / np.std(centered_tX, axis=0)\n",
    "\n",
    "d=len(standardized_tX[0])\n",
    "n=len(standardized_tX)\n",
    "mat=np.zeros((n,int(d*(d+1)/2+d)))\n",
    "q=0\n",
    "tao=d \n",
    "\n",
    "for i in range (n):\n",
    "    for t in range (d):  \n",
    "        mat[i,t]=standardized_tX[i,t]\n",
    "\n",
    "        \n",
    "for i in range (n):\n",
    "    for monsoon in range (d):\n",
    "        for t in range (monsoon,d): \n",
    "            mat[i,tao]=standardized_tX[i,t]*standardized_tX[i,q]\n",
    "            tao=tao+1\n",
    "        q=q+1    \n",
    "    tao=d  \n",
    "    q=0\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples=len(mat)\n",
    "tx = np.c_[np.ones(num_samples), mat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # least squares: TODO\n",
    "    # returns mse, and optimal weights\n",
    "    xtx=np.dot(tx.transpose(),tx)\n",
    "    xy=np.dot(tx.transpose(),y)\n",
    "    w=np.dot(np.linalg.inv(xtx),xy)\n",
    "    #e=y-np.dot(tx,w)\n",
    "    #mse=np.dot(e.transpose(),e)/(2*len(tx))\n",
    "    return w\n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression: TODO\n",
    "    xtx=np.dot(tx.transpose(),tx)\n",
    "    l_inside=2*lamb*len(tx)*np.eye(tx.shape[1])\n",
    "    ins=xtx+l_inside\n",
    "    xy=np.dot(tx.transpose(),y)\n",
    "    \n",
    "    w=np.dot(np.linalg.inv(ins),xy)\n",
    "    e=y-np.dot(tx,w)\n",
    "    mse=np.dot(e.transpose(),e)/(2*len(tx))\n",
    "    \n",
    "    return w\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "    \n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    e=y-np.dot(tx,w)\n",
    "    mse=np.dot(e.transpose(),e)/(2*len(tx))\n",
    "    return mse\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    e=y-np.dot(tx,w)\n",
    "    return (-1/len(tx))*np.dot(tx.transpose(),e)\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, gamma, max_iters):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    initial_w=np.zeros(tx.shape[1])\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        grad=compute_gradient(y,tx,w)\n",
    "        loss=compute_loss(y,tx,w)\n",
    "        # ***************************************************\n",
    "        gamma=gamma/1.005\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        w=w-gamma*grad\n",
    "        # ***************************************************\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=0.5, w0=-6.28112593299285e-05, w1=-1.068336422568577e-05\n",
      "Gradient Descent(1/99): loss=0.4982547749351945, w0=-0.00012503951243584744, w1=-2.0763507914137518e-05\n",
      "Gradient Descent(2/99): loss=0.496595316790688, w0=-0.00018670535384794415, w1=-3.0266843521689202e-05\n",
      "Gradient Descent(3/99): loss=0.49501318489875906, w0=-0.00024782536563590243, w1=-3.921761504412473e-05\n",
      "Gradient Descent(4/99): loss=0.4935025167838092, w0=-0.00030841363238479254, w1=-4.763843034319548e-05\n",
      "Gradient Descent(5/99): loss=0.492058457624324, w0=-0.0003684825674529154, w1=-5.5550569442658576e-05\n",
      "Gradient Descent(6/99): loss=0.4906766938390041, w0=-0.0004280433793418558, w1=-6.297417456191304e-05\n",
      "Gradient Descent(7/99): loss=0.4893532963011151, w0=-0.0004871063476367299, w1=-6.99283764883267e-05\n",
      "Gradient Descent(8/99): loss=0.4880846533351708, w0=-0.0005456809963989478, w1=-7.643138569033479e-05\n",
      "Gradient Descent(9/99): loss=0.4868674318005186, w0=-0.0006037762109362137, w1=-8.250056309027842e-05\n",
      "Gradient Descent(10/99): loss=0.4856985485263058, w0=-0.0006614003221833156, w1=-8.815247843842891e-05\n",
      "Gradient Descent(11/99): loss=0.4845751467324044, w0=-0.0007185611716502908, w1=-9.340296059583424e-05\n",
      "Gradient Descent(12/99): loss=0.4834945756184159, w0=-0.000775266163993839, w1=-9.826714212663373e-05\n",
      "Gradient Descent(13/99): loss=0.4824543723588211, w0=-0.0008315223111559648, w1=-0.00010275949958980722\n",
      "Gradient Descent(14/99): loss=0.48145224608341214, w0=-0.000887336270355948, w1=-0.00010689389037746437\n",
      "Gradient Descent(15/99): loss=0.48048606355209256, w0=-0.0009427143773262833, w1=-0.00011068358664966084\n",
      "Gradient Descent(16/99): loss=0.4795538362968276, w0=-0.0009976626756901627, w1=-0.00011414130674871221\n",
      "Gradient Descent(17/99): loss=0.478653709043568, w0=-0.0010521869430991648, w1=-0.00011727924437853093\n",
      "Gradient Descent(18/99): loss=0.4777839492565091, w0=-0.0011062927145860137, w1=-0.00012010909577478646\n",
      "Gradient Descent(19/99): loss=0.476942937670535, w0=-0.0011599853034863492, w1=-0.00012264208505281162\n",
      "Gradient Descent(20/99): loss=0.47612915969702124, w0=-0.0012132698202174314, w1=-0.00012488898789313107\n",
      "Gradient Descent(21/99): loss=0.47534119760423904, w0=-0.001266151189155763, w1=-0.00012686015370441626\n",
      "Gradient Descent(22/99): loss=0.474577723387107, w0=-0.0013186341638215227, w1=-0.0001285655263879065\n",
      "Gradient Descent(23/99): loss=0.4738374922523911, w0=-0.0013707233405510718, w1=-0.0001300146638143999\n",
      "Gradient Descent(24/99): loss=0.4731193366550715, w0=-0.0014224231708170713, w1=-0.0001312167561139501\n",
      "Gradient Descent(25/99): loss=0.47242216082975186, w0=-0.0014737379723374702, w1=-0.00013218064286890736\n",
      "Gradient Descent(26/99): loss=0.4717449357679241, w0=-0.0015246719390989436, w1=-0.00013291482929258876\n",
      "Gradient Descent(27/99): loss=0.4710866945978465, w0=-0.0015752291504066943, w1=-0.0001334275014684501\n",
      "Gradient Descent(28/99): loss=0.47044652832886424, w0=-0.0016254135790605374, w1=-0.00013372654071801062\n",
      "Gradient Descent(29/99): loss=0.4698235819263909, w0=-0.001675229098746548, w1=-0.00013381953715984097\n",
      "Gradient Descent(30/99): loss=0.4692170506875401, w0=-0.0017246794907241822, w1=-0.00013371380251658122\n",
      "Gradient Descent(31/99): loss=0.4686261768906696, w0=-0.0017737684498803537, w1=-0.00013341638222213572\n",
      "Gradient Descent(32/99): loss=0.4680502466949459, w0=-0.0018224995902145211, w1=-0.00013293406687683897\n",
      "Gradient Descent(33/99): loss=0.4674885872685237, w0=-0.0018708764498121856, w1=-0.00013227340309444942\n",
      "Gradient Descent(34/99): loss=0.46694056412609436, w0=-0.0019189024953582025, w1=-0.0001314407037812612\n",
      "Gradient Descent(35/99): loss=0.4664055786584866, w0=-0.0019665811262360683, w1=-0.00013044205788439097\n",
      "Gradient Descent(36/99): loss=0.46588306583865974, w0=-0.002013915678254541, w1=-0.00012928333964336186\n",
      "Gradient Descent(37/99): loss=0.4653724920899358, w0=-0.002060909427038723, w1=-0.00012797021737643885\n",
      "Gradient Descent(38/99): loss=0.46487335330362445, w0=-0.002107565591118935, w1=-0.00012650816183074556\n",
      "Gradient Descent(39/99): loss=0.46438517299436805, w0=-0.0021538873347472947, w1=-0.0001249024541229815\n",
      "Gradient Descent(40/99): loss=0.46390750058258373, w0=-0.002199877770468885, w1=-0.00012315819329554773\n",
      "Gradient Descent(41/99): loss=0.463439909794306, w0=-0.002245539961471641, w1=-0.00012128030351105035\n",
      "Gradient Descent(42/99): loss=0.4629819971695808, w0=-0.0022908769237366604, w1=-0.00011927354090647413\n",
      "Gradient Descent(43/99): loss=0.4625333806713106, w0=-0.0023358916280084296, w1=-0.00011714250012678537\n",
      "Gradient Descent(44/99): loss=0.46209369838712583, w0=-0.0023805870016025076, w1=-0.00011489162055631754\n",
      "Gradient Descent(45/99): loss=0.46166260731747855, w0=-0.0024249659300664373, w1=-0.00011252519226500903\n",
      "Gradient Descent(46/99): loss=0.46123978224370293, w0=-0.002469031258708063, w1=-0.00011004736168538162\n",
      "Gradient Descent(47/99): loss=0.4608249146702916, w0=-0.0025127857940040257, w1=-0.00010746213703506555\n",
      "Gradient Descent(48/99): loss=0.46041771183609376, w0=-0.002556232304899931, w1=-0.00010477339349868189\n",
      "Gradient Descent(49/99): loss=0.4600178957895591, w0=-0.0025993735240125194, w1=-0.00010198487818197753\n",
      "Gradient Descent(50/99): loss=0.45962520252352496, w0=-0.0026422121487431655, w1=-9.910021485026479e-05\n",
      "Gradient Descent(51/99): loss=0.45923938116539426, w0=-0.0026847508423110946, w1=-9.612290846243989e-05\n",
      "Gradient Descent(52/99): loss=0.458860193218867, w0=-0.0027269922347138774, w1=-9.305634951113784e-05\n",
      "Gradient Descent(53/99): loss=0.4584874118536766, w0=-0.002768938923622008, w1=-8.990381817891814e-05\n",
      "Gradient Descent(54/99): loss=0.4581208212400475, w0=-0.0028105934752137044, w1=-8.666848831976308e-05\n",
      "Gradient Descent(55/99): loss=0.4577602159248336, w0=-0.0028519584249554852, w1=-8.335343127460293e-05\n",
      "Gradient Descent(56/99): loss=0.457405400246523, w0=-0.002893036278333487, w1=-7.996161952905699e-05\n",
      "Gradient Descent(57/99): loss=0.4570561877864933, w0=-0.00293382951154005, w1=-7.64959302210917e-05\n",
      "Gradient Descent(58/99): loss=0.45671240085410225, w0=-0.002974340572119614, w1=-7.295914850584456e-05\n",
      "Gradient Descent(59/99): loss=0.456373870003357, w0=-0.0030145718795776054, w1=-6.935397078444144e-05\n",
      "Gradient Descent(60/99): loss=0.45604043357908325, w0=-0.0030545258259556166, w1=-6.568300780324468e-05\n",
      "Gradient Descent(61/99): loss=0.45571193729064696, w0=-0.003094204776375858, w1=-6.194878762960337e-05\n",
      "Gradient Descent(62/99): loss=0.45538823381143223, w0=-0.003133611069557597, w1=-5.815375850983924e-05\n",
      "Gradient Descent(63/99): loss=0.4550691824023943, w0=-0.003172747018308006, w1=-5.430029161488369e-05\n",
      "Gradient Descent(64/99): loss=0.45475464855813025, w0=-0.003211614909989615, w1=-5.039068367868646e-05\n",
      "Gradient Descent(65/99): loss=0.4544445036740172, w0=-0.003250217006966381, w1=-4.642715953423884e-05\n",
      "Gradient Descent(66/99): loss=0.45413862473306316, w0=-0.0032885555470301433, w1=-4.2411874551797474e-05\n",
      "Gradient Descent(67/99): loss=0.45383689401121446, w0=-0.0033266327438091063, w1=-3.8346916983651834e-05\n",
      "Gradient Descent(68/99): loss=0.4535391987999452, w0=-0.0033644507871598162, w1=-3.4234310219550865e-05\n",
      "Gradient Descent(69/99): loss=0.4532454311450365, w0=-0.0034020118435439564, w1=-3.0076014956693217e-05\n",
      "Gradient Descent(70/99): loss=0.45295548760052484, w0=-0.003439318056391163, w1=-2.5873931287985965e-05\n",
      "Gradient Descent(71/99): loss=0.45266926899686977, w0=-0.003476371546448964, w1=-2.1629900712085638e-05\n",
      "Gradient Descent(72/99): loss=0.45238668022245204, w0=-0.0035131744121207993, w1=-1.7345708068566383e-05\n",
      "Gradient Descent(73/99): loss=0.4521076300175742, w0=-0.0035497287297930396, w1=-1.3023083401385398e-05\n",
      "Gradient Descent(74/99): loss=0.45183203078019235, w0=-0.0035860365541517942, w1=-8.663703753668739e-06\n",
      "Gradient Descent(75/99): loss=0.4515597983826531, w0=-0.0036220999184902545, w1=-4.269194896685653e-06\n",
      "Gradient Descent(76/99): loss=0.45129085199876184, w0=-0.0036579208350072242, w1=1.5886700425204697e-07\n",
      "Gradient Descent(77/99): loss=0.45102511394055594, w0=-0.003693501295097444, w1=4.618953784365526e-06\n",
      "Gradient Descent(78/99): loss=0.45076250950418395, w0=-0.0037288432696342584, w1=9.109583731978157e-06\n",
      "Gradient Descent(79/99): loss=0.4505029668243495, w0=-0.0037639487092451154, w1=1.3629320096716871e-05\n",
      "Gradient Descent(80/99): loss=0.4502464167367965, w0=-0.0037988195445803536, w1=1.8176769651092272e-05\n",
      "Gradient Descent(81/99): loss=0.4499927926483564, w0=-0.0038334576865756836, w1=2.2750581303308417e-05\n",
      "Gradient Descent(82/99): loss=0.44974203041410543, w0=-0.003867865026708736, w1=2.7349444759254228e-05\n",
      "Gradient Descent(83/99): loss=0.4494940682212086, w0=-0.0039020434372500122, w1=3.197208923171644e-05\n",
      "Gradient Descent(84/99): loss=0.44924884647905267, w0=-0.003935994771508551, w1=3.66172821949487e-05\n",
      "Gradient Descent(85/99): loss=0.4490063077153003, w0=-0.0039697208640725856, w1=4.1283828182810394e-05\n",
      "Gradient Descent(86/99): loss=0.44876639647751615, w0=-0.004003223531045448, w1=4.59705676287709e-05\n",
      "Gradient Descent(87/99): loss=0.44852905924003694, w0=-0.004036504570276964, w1=5.067637574614804e-05\n",
      "Gradient Descent(88/99): loss=0.44829424431578485, w0=-0.004069565761590528, w1=5.540016144702397e-05\n",
      "Gradient Descent(89/99): loss=0.4480619017727321, w0=-0.0041024088670060825, w1=6.014086629834693e-05\n",
      "Gradient Descent(90/99): loss=0.44783198335475405, w0=-0.004135035630959158, w1=6.489746351379299e-05\n",
      "Gradient Descent(91/99): loss=0.44760444240661307, w0=-0.004167447780516144, w1=6.966895698002432e-05\n",
      "Gradient Descent(92/99): loss=0.4473792338028416, w0=-0.004199647025585947, w1=7.445438031603599e-05\n",
      "Gradient Descent(93/99): loss=0.44715631388029675, w0=-0.004231635059128159, w1=7.925279596434301e-05\n",
      "Gradient Descent(94/99): loss=0.4469356403741844, w0=-0.004263413557357877, w1=8.40632943128068e-05\n",
      "Gradient Descent(95/99): loss=0.4467171723573525, w0=-0.004294984179947282, w1=8.888499284595684e-05\n",
      "Gradient Descent(96/99): loss=0.44650087018267043, w0=-0.004326348570224091, w1=9.37170353247047e-05\n",
      "Gradient Descent(97/99): loss=0.4462866954283223, w0=-0.004357508355366971, w1=9.855859099339918e-05\n",
      "Gradient Descent(98/99): loss=0.446074610845852, w0=-0.004388465146598017, w1=0.00010340885381321055\n",
      "Gradient Descent(99/99): loss=0.4458645803108058, w0=-0.004419220539372379, w1=0.00010826704172087549\n",
      "Gradient Descent: execution time=16.443 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "import datetime\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.0002\n",
    "\n",
    "# Initialization\n",
    "#w_initial = np.array([0.0, 0.0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = least_squares_GD(y, tx, gamma, max_iters)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "least_squares_w=least_squares(y,tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lamb=0.00075\n",
    "ridge_w=ridge_regression(y, tx, lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testx[testx==-999]=0\n",
    "m2=np.mean(testx,axis=0)\n",
    "centered_testx=testx-m2\n",
    "centered_testx[centered_testx==-m2]=0\n",
    "standardized_testx=centered_testx / np.std(centered_testx, axis=0)\n",
    "\n",
    "\n",
    "d=len(standardized_testx[0])\n",
    "n=len(standardized_testx)\n",
    "mat=np.zeros((n,int(d*(d+1)/2+d)))\n",
    "q=0\n",
    "tao=d \n",
    "\n",
    "for i in range (n):\n",
    "    for t in range (d):  \n",
    "        mat[i,t]=standardized_testx[i,t]\n",
    "\n",
    "        \n",
    "for i in range (n):\n",
    "    for mk in range (d):\n",
    "        for t in range (mk,d): \n",
    "            mat[i,tao]=standardized_testx[i,t]*standardized_testx[i,q]\n",
    "            tao=tao+1\n",
    "        q=q+1    \n",
    "    tao=d  \n",
    "    q=0\n",
    "\n",
    "    \n",
    "num_samples=len(standardized_testx)\n",
    "final_testx = np.c_[np.ones(num_samples), mat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79897597952\n"
     ]
    }
   ],
   "source": [
    "w=ridge_w\n",
    "#w=least_squares_w\n",
    "#w=gradient_ws[-1]\n",
    "aq=np.dot(final_testx,w)\n",
    "y_guess=np.sign(aq)\n",
    "trueness=sum((y_guess==testy))/len(testy)\n",
    "print(trueness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '/Users/alperkose/Desktop/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testx=tX_test\n",
    "testx[testx==-999]=0\n",
    "m2=np.mean(testx,axis=0)\n",
    "centered_testx=testx-m2\n",
    "centered_testx[centered_testx==-m2]=0\n",
    "standardized_testx=centered_testx / np.std(centered_testx, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "d=len(standardized_testx[0])\n",
    "n=len(standardized_testx)\n",
    "mat=np.zeros((n,d*(d+1)/2+d))\n",
    "q=0\n",
    "tao=d \n",
    "\n",
    "for i in range (n):\n",
    "    for t in range (d):  \n",
    "        mat[i,t]=standardized_testx[i,t]\n",
    "\n",
    "        \n",
    "for i in range (n):\n",
    "    for m in range (d):\n",
    "        for t in range (m,d): \n",
    "            mat[i,tao]=standardized_testx[i,t]*standardized_testx[i,q]\n",
    "            tao=tao+1\n",
    "        q=q+1    \n",
    "    tao=d  \n",
    "    q=0\n",
    "\n",
    "\n",
    "num_samples=len(standardized_testx)\n",
    "final_testx = np.c_[np.ones(num_samples), mat]\n",
    "tX_test=final_testx\n",
    "weights=ridge_w\n",
    "#tX_test = np.c_[np.ones(len(tX_test)), tX_test]\n",
    "#weights=gradient_ws[-1]\n",
    "#w=gradient_ws[-1]\n",
    "#aq=np.dot(tX_test,w)\n",
    "#y_guess=np.sign(aq)\n",
    "#trueness=sum((y_guess==y))/y.shape[0]\n",
    "#print(trueness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/Users/alperkose/Desktop/deneme8.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
